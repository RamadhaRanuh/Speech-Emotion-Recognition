Speech emotion recognition (SER) is an important area of research in human-computer interaction. The ability to recognize emotions from speech can improve user experience in applications such as virtual assistants, automated customer service, and sentiment analysis. Emotions affect the way we speak, and this is reflected in various acoustic features of the voice such as pitch, intensity, and rhythm. However, challenges in emotion recognition from speech include variations in the expression of emotions by different individuals and diverse recording conditions. In this project, we use Convolutional Neural Networks (CNN) to identify emotions from audio recordings, by comparing the performance of the model before and after the application of data augmentation techniques to improve the accuracy and generalization of the model.
