# Speech Emotion Recognition using CNN with Data Augmentation

This project focuses on recognizing emotions from speech using Convolutional Neural Networks (CNN). The ability to identify emotions from audio recordings can enhance user experience in applications such as virtual assistants, automated customer service, and sentiment analysis. Emotions influence various acoustic features of speech, such as pitch, intensity, and rhythm. This project compares the performance of the CNN model before and after applying data augmentation techniques to improve accuracy and generalization.

## Table of Contents

- [Introduction](#introduction)
- [Dataset](#dataset)
- [Methods](#methods)
  - [CNN](#cnn)
  - [Data Augmentation](#data-augmentation)
- [Implementation](#implementation)
  - [Preprocessing](#preprocessing)
  - [Model Training](#model-training)
  - [Evaluation](#evaluation)
- [Results](#results)
- [Conclusion](#conclusion)

## Introduction

Speech Emotion Recognition (SER) is an important area of research in human-computer interaction. Emotions affect the way we speak, and this is reflected in various acoustic features of the voice such as pitch, intensity, and rhythm. Challenges in emotion recognition from speech include variations in the expression of emotions by different individuals and diverse recording conditions. This project utilizes Convolutional Neural Networks (CNN) to identify emotions from audio recordings and examines the impact of data augmentation techniques on model performance.

## Dataset

The dataset used in this project consists of audio recordings annotated with emotional labels. The recordings are preprocessed to extract relevant acoustic features, which are then used to train the CNN model. Data augmentation techniques such as noise addition and pitch changes are applied to create variations in the training data.

## Methods

### CNN

Convolutional Neural Networks (CNN) are employed to learn and identify emotion patterns from the audio features. The CNN architecture is designed to capture the temporal and spectral properties of the audio signals.

### Data Augmentation

Data augmentation techniques are used to increase the diversity of the training data, helping the model to generalize better to new data. Techniques such as noise addition, pitch changes, and time stretching are applied to create augmented versions of the original recordings.

## Implementation

### Preprocessing

The audio recordings are preprocessed to extract features such as Mel-frequency cepstral coefficients (MFCCs), which are used as input to the CNN model. Data augmentation is applied during this phase to enhance the training dataset.

### Model Training

The CNN model is trained on the preprocessed and augmented data. The training process involves optimizing the model parameters to minimize the loss function and improve accuracy.

### Evaluation

The performance of the model is evaluated on a separate test set. Metrics such as accuracy and loss are used to compare the models trained with and without data augmentation.

## Results

The evaluation results show that the CNN model trained with augmented data performs significantly better than the model trained without augmentation. The following are the details of the evaluation results:

1. **Model with Augmented Data:**
   - Train Loss: 0.0170
   - Train Accuracy: 99.57%
   - Validation Loss: 2.0879
   - Validation Accuracy: 58.47%
   - Test Loss: 2.0532
   - Test Accuracy: 60.64%

2. **Model without Data Augmentation:**
   - Train Loss: 0.0274
   - Train Accuracy: 99.59%
   - Validation Loss: 1.9010
   - Validation Accuracy: 57.48%
   - Test Loss: 2.1423
   - Test Accuracy: 56.2%

Analysis of these results indicates that data augmentation is effective in improving model performance. The model trained with augmented data achieved a test accuracy of 60.64%, compared to 56.2% for the model without augmentation. The loss generated by the model with augmentation is also lower (2.0532) than the model without augmentation (2.1423), indicating better minimization of prediction errors. However, the difference in loss between training and validation suggests some overfitting in the model.

## Conclusion

The conclusion of this project is that Convolutional Neural Networks (CNN) can effectively be used for emotion recognition from speech with high accuracy, especially when data augmentation is applied. Data augmentation techniques significantly improve model performance, with the augmented model achieving a test accuracy of 60.64% compared to 56.2% for the model without augmentation. This confirms the importance of data variation in training deep learning models to improve generalization ability and model accuracy. However, the difference in loss between training and validation indicates a need for further regularization to reduce overfitting.


